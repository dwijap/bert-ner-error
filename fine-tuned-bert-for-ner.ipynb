{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import utilities as utils\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from scipy import stats\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model predictions (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1303188\n",
      "-rw-r--r-- 1 dwija dwija       1760 Jun 15 10:17 config.json\n",
      "-rw-r--r-- 1 dwija dwija 1334453533 Jun 15 10:17 pytorch_model.bin\n",
      "-rw-r--r-- 1 dwija dwija       1977 Jun 15 10:17 training_meta.bin\n",
      "total 4\n",
      "-rw-r--r-- 1 dwija dwija 1884 Jun 15 10:17 b1.0-conll03-large.json\n"
     ]
    }
   ],
   "source": [
    "conll03_config = \"b1.0-conll03-large\"\n",
    "\n",
    "!ls -l results/{conll03_config}/checkpoint/\n",
    "!ls -l configs/baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I saved the best predictions of the model in the checkpoint folder (check them out `ls results/b1.0-conll03-large/*.bin`). You can load the predictions along with the lables from those binary files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/b1.0-conll03-large/dev_best_preds.bin\n",
      "results/b1.0-conll03-large/dev_preds_across_epochs.bin\n",
      "results/b1.0-conll03-large/test_best_preds.bin\n",
      "results/b1.0-conll03-large/train_preds_across_epochs.bin\n"
     ]
    }
   ],
   "source": [
    "!ls results/b1.0-conll03-large/*.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes      precision    recall    f1-score    support\n",
      "---------  -----------  --------  ----------  ---------\n",
      "LOC             92.679    93.345      93.011       1668\n",
      "MISC            82.464    81.054      81.753        702\n",
      "ORG             85.125    91.993      88.426       1661\n",
      "PER             96.669    95.114      95.885       1617\n",
      "\n",
      "macro avg       90.330    91.926      91.086       5648\n",
      "micro avg       90.202    91.926      91.056       5648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from utilities import EpochStats\n",
    "\n",
    "conll03_args = utils.load_args(default_config=f\"configs/baselines/{conll03_config}.json\")\n",
    "\n",
    "test_pred_stats = torch.load(f\"results/{conll03_config}/test_best_preds.bin\")\n",
    "test_pred_stats.print_classification_report(conll03_args.data.label_scheme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can load the predictions, convert them to label (using label_scheme), and then load the actual data (e.g., from the data folder) and align them to check the results. Both ways are equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes      precision    recall    f1-score    support\n",
      "---------  -----------  --------  ----------  ---------\n",
      "LOC             92.679    93.345      93.011       1668\n",
      "MISC            82.464    81.054      81.753        702\n",
      "ORG             85.125    91.993      88.426       1661\n",
      "PER             96.669    95.114      95.885       1617\n",
      "\n",
      "macro avg       90.330    91.926      91.086       5648\n",
      "micro avg       90.202    91.926      91.056       5648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import f1_score, precision_score, recall_score\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "conll03_bert_pred = {\n",
    "    \"ner\": test_pred_stats._map_to_labels(conll03_args.data.label_scheme)[1]\n",
    "}\n",
    "\n",
    "conll03_test = os.path.join(conll03_args.data.directory, \"test.txt\")\n",
    "conll03_test = utils.read_conll(conll03_test, columns={'txt': 0, 'ner': 1})\n",
    "\n",
    "utils.printcr(\n",
    "    utils.report2dict(\n",
    "        classification_report(conll03_test['ner'], conll03_bert_pred['ner'], digits=5)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pred = {\n",
    "    \"pred\": test_pred_stats._map_to_labels(conll03_args.data.label_scheme)[1]\n",
    "}\n",
    "bert_token = {\n",
    "    \"tokens\": test_pred_stats._map_to_labels(conll03_args.data.label_scheme)[0]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import BertConfig, BertForTokenClassification, BertTokenizer\n",
    "from nets import NERModel\n",
    "\n",
    "def load_model(checkpoint_dir, args):\n",
    "    tokenizer = BertTokenizer.from_pretrained(args.model.pretrained)\n",
    "    model = NERModel.from_pretrained(checkpoint_dir)\n",
    "    model.eval()\n",
    "    labels = args.data.label_scheme\n",
    "    return model, tokenizer, labels\n",
    "\n",
    "model, tokenizer, index2label = load_model(f\"results/{conll03_config}/checkpoint/\", conll03_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the `predict` function that uses the previously-loaded models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(tokens, verbose=False):\n",
    "    # Including special tokens\n",
    "    input_ids = []\n",
    "    label_msk = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        bert_tokenized = tokenizer.encode(token, add_special_tokens=False)\n",
    "        if len(bert_tokenized) == 1:\n",
    "            input_ids.extend(bert_tokenized)\n",
    "            label_msk.append(1)\n",
    "        else:\n",
    "            input_ids.extend(bert_tokenized)\n",
    "            label_msk.extend([1] + [0] * (len(bert_tokenized) - 1))\n",
    "    \n",
    "    input_ids = torch.tensor([ [tokenizer.cls_token_id] + input_ids ])\n",
    "    label_msk = torch.tensor([ [0] + label_msk ])\n",
    "    \n",
    "    if verbose:\n",
    "        print(tokens)\n",
    "        print(tokenizer.tokenize(' '.join(tokens)))\n",
    "    \n",
    "    assert input_ids.shape == label_msk.shape\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        losses, scores = model(input_ids, attention_mask=None, token_type_ids=None)\n",
    "        scores = scores[label_msk == 1]\n",
    "        logits = torch.softmax(scores, dim=-1)\n",
    "        classes = torch.argmax(logits, dim=-1).view(-1)\n",
    "        preds = [index2label[ix] for ix in classes] \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using the model\n",
    "\n",
    "We can try the fine-tuned modal now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-PER', 'I-PER', 'O', 'O', 'O']\n",
      "['B-PER', 'O', 'O', 'O', 'O']\n",
      "['B-ORG', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(predict(\"Barack Obama was a president\".split()))\n",
    "print(predict(\"Gustavo plays at the park\".split()))\n",
    "print(predict(\"Solorio's speech was inspired\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['B-PER', 'I-PER', 'O', 'O', 'O'],\n",
       " ['B-PER', 'I-PER', 'O', 'O', 'O'],\n",
       " ['B-ORG', 'I-ORG', 'O', 'O', 'O'],\n",
       " ['B-PER', 'I-PER'],\n",
       " ['O', 'O'],\n",
       " ['B-ORG', 'O']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_many(sentences):\n",
    "    predictions = []\n",
    "    for s in sentences:\n",
    "        predictions.append(predict(s))\n",
    "    return predictions\n",
    "    \n",
    "predict_many([\n",
    "    'Barack Obama was a president'.split(),\n",
    "    'barack obama was a president'.split(),\n",
    "    'bArAcK oBaMa was a president'.split(),\n",
    "    'Barack Obama'.split(),\n",
    "    'barack obama'.split(),\n",
    "    'bArAcK oBaMa'.split(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC'],\n",
       " ['B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
       " ['O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC'],\n",
       " ['B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC'],\n",
       " ['O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC'],\n",
       " ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC'],\n",
       " ['B-LOC', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_many([\n",
    "    'China will be present at the Winter Olympics'.split(),\n",
    "    'China will be present at the winter olympics'.split(),\n",
    "    'china will be present at the Winter Olympics'.split(),\n",
    "    'Russia will be present at the Winter Olympics'.split(),\n",
    "    'The USA will be present at the Winter Olympics'.split(),\n",
    "    'the usa will be present at the Winter Olympics'.split(),\n",
    "    'USA will be present at the Winter Olympics'.split(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['O', 'O', 'O', 'O', 'O', 'O'],\n",
       " ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
       " ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
       " ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_many([\n",
    "    'I will not go Outside today'.split(),\n",
    "    'I will not go to the outside today'.split(),\n",
    "    'I will not go to the Outside today'.split(),\n",
    "    'I will not go to the Outside Today'.split(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['B-ORG',\n",
       "  'O',\n",
       "  'B-MISC',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-MISC',\n",
       "  'I-MISC',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O'],\n",
       " ['O', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O'],\n",
       " ['B-PER', 'O', 'O', 'O', 'O', 'B-MISC', 'I-MISC', 'O', 'O', 'O', 'O']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_many([\n",
    "    'Microsoftâ€™s new Edge browser will be built into Windows 10 in the update to be released in August'.split(),\n",
    "    'julia will be built into Windows 10 again later this year'.split(),\n",
    "    'Julia will be built into Windows 10 again later this year'.split(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
